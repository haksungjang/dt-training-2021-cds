# -*- coding: utf-8 -*-
"""CDS리그도전_0726_V1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nlny3JVc5bYHU82zGQpqtMcrGvzLWxkB

# **CDS리그 도전중**

## **1. 학습 환경 세팅**

> ### 0) 코랩: 구글 드라이브 연동
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# 본인이 파일 업로드 한 폴더로 경로 지정
# %cd /content/drive/MyDrive/CDS_Files

"""> ### 1) 학습 모델 선택"""

# Model Activity or No (0: deactivation, 1: activation)
b_lr_active      = 0  ## LogisticRegression
b_tree_active    = 0  ## DecisionTreeClassifier
b_voting_active  = 0  ## VotingClassifier
b_rfc_active     = 0  ## RandomForestClassifier
b_gbc_active     = 0  ## GradientBoostingClassifier
b_xgb_active     = 0  ## XGBClassifier 
b_lgbm_active    = 1  ## LGBMClassifier 
b_stack_active   = 0  ## StackingClassifier 
# function on/off RandomSearchCV / GridSearchCV
b_randomSCV_lgbm = 0  ## RandomSearchCV, LGBMClassifier
b_randomSCV_xgb  = 0  ## RandomSearchCV, XGBClassifier
b_gridSCV_lgbm   = 0  ## GridSearchCV, LGBMClassifier
b_gridSCV_xgb    = 0  ## GridSearchCV, XGBClassifier
b_bayesianOpt    = 0  ## BayesianOptimization

"""> ### 2) Import Package """

## 주요 라이브러리 import
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from dateutil.relativedelta import relativedelta
from datetime import *

"""## **2. 데이터 읽기**

> ### 1) Train 데이터 로드
"""

# 서비스 데이터 읽기 : 전체 69,708 rows
# 고객 상품(이용권)별 재결제(해지) 이력 정보
ds_service = "train_service.csv"
df_service = pd.read_csv(ds_service, parse_dates=['registerdate','enddate'], infer_datetime_format=True)

# 데이터에 대한 전반적인 정보를 표시 dataframe을 구성하는 행과 열의 크기, 컬럼명, 컬럼을 구성하는 값의 자료형 등을 출력
df_service.info()
# train_service.csv 데이터 샘플 3개 출력
df_service.sample(3)

# 시청 이력(train_bookmark) 데이터 읽기 : 7,987,609 rows
ds_bookmark = "train_bookmark.csv"
df_bookmark = pd.read_csv(ds_bookmark, parse_dates=['dates'], infer_datetime_format=True)

# 데이터에 대한 전반적인 정보를 표시 dataframe을 구성하는 행과 열의 크기, 컬럼명, 컬럼을 구성하는 값의 자료형 등을 출력
df_bookmark.info()
# 데이터 샘플 3개 출력
df_bookmark.sample(3)

# Coin 이용 결제 종류/방법 파일 coin.csv 데이터 읽기 : 전체 11,073 rows
ds_coin = "coin.csv"
df_coin = pd.read_csv(ds_coin, parse_dates=['registerdate'], infer_datetime_format=True)

# 데이터에 대한 전반적인 정보를 표시 dataframe을 구성하는 행과 열의 크기, 컬럼명, 컬럼을 구성하는 값의 자료형 등을 출력
df_coin.info()
# 데이터 샘플 3개 출력
df_coin.sample(3)

# service 파일 컬럼별 unique values 확인
for column in df_service.columns.values.tolist():
    print(column)
    print(df_service[column].unique())
    print("")

# bookmark 파일 컬럼별 unique values 확인 
for column in df_bookmark.columns.values.tolist():
    print(column)
    print(df_bookmark[column].unique())
    print("")

# 2021.07.05 by DH
# coin 파일 컬럼별 unique values 확인
for column in df_coin.columns.values.tolist():
    print(column)
    print(df_coin[column].unique())
    print("")

"""## **3. EDA (탐색적 데이터 분석)**

> ### 1) 결측치 등 데이터 전처리
"""

# 결과 중 결측치 비율을 보고 어느 컬럼을 삭제하고 어느 컬럼을 대체할지를 판단 
df_missing = df_service
np.sum(df_missing.isnull())
missing_number = df_missing.isnull().sum().sort_values(ascending=False)
missing_percentage = missing_number/len(df_missing)
missing_info = pd.concat([missing_number,missing_percentage], axis=1, keys=['missing number','missing percentage'])
missing_info.head(50)

# gender 값 및 분포 확인
print('gender = ', df_service.gender.unique().tolist())
df_service.gender.value_counts()

# gender null 값을 N 으로 변경 후 확인
df_service['gender'] = df_service['gender'].fillna('N')
print('gender = ', sorted(df_service.gender.unique().tolist()))
df_service.gender.value_counts()

# agegroup 값 및 분포 확인
print('agegroup = ', sorted(df_service.agegroup.unique().tolist()))
df_service.agegroup.value_counts()

# 2021.07.05 by DH
# agegroup 120 이상 값을 0 으로 변환
df_service.loc[df_service['agegroup'] >= 120, 'agegroup'] = 0

print('agegroup = ', sorted(df_service.agegroup.unique().tolist()))
df_service.agegroup.value_counts()

# pgamount 값 및 분포 확인
print('pgamount = ', sorted(df_service.pgamount.unique().tolist()))
df_service.pgamount.value_counts()

# pgamount 금액 중에 달러로 결제된 것 원화로 변경 (pgamount 100원 미만인 건은 Appstore에서 달러 결제 건임)
df_service.loc[(df_service['pgamount'] <  100), 'pgamount'] = df_service['pgamount'] * 1120
print('pgamount = ', sorted(df_service.pgamount.unique().tolist()))
df_service.pgamount.value_counts()

# 기타 컬럼들의 결측치 처리
df_service = df_service.fillna('X')
# 결측치 유무 최종 확인
df_service.info()

"""> ### 2) 데이터 생성/가공  """

# 해지 예측 대상 서비스 추출
# 즉, 예측시점(가입일+3주)에 해지하지 않은 서비스 추출
df_svc_target = df_service[df_service.enddate.dt.date > (df_service.registerdate + pd.DateOffset(weeks=3)).dt.date]

# 즉, 예측시점(가입일+3주)에 해지하지 않은 서비스 개수 확인
print('df_svc_target = {0:,}'.format(df_svc_target.shape[0]), 'rows')

# 경연 운영진 제공 정보 이용 - service 파일 productcode 데이터 파싱
# productcode 별 결제방법(payment) 및 시청기기(mobile, pc, tv) 등 데이터 테이블 만들기 

idx_pcode = sorted(df_service.productcode.unique().tolist(), reverse=False)
print('productcode = ', len(sorted(df_service.productcode.unique().tolist())))
# 결제(PAYMENT) : 1M(자동결제/월정액), 30D(30일/이용권), PRM(프로모션)
payment_list = ['1M','1M','1M','1M','1M','1M','1M','1M','1M','1M',
                '1M','1M','1M','1M','1M','1M','1M','1M','1M','1M',
                '1M','1M','1M','1M','1M','1M','1M','1M','30D','30D',
                '30D','1M','1M','1M','1M','1M','30D','1M','30D','30D',
                '30D','30D','1M','1M','1M','1M','1M','1M','1M','1M',
                '1M','1M','1M','1M','1M','1M','1M','30D','30D','30D',
                'PRM','PRM','1M','PRM','1M','1M','1M','1M','1M','1M',
                '30D','30D','1M','1M','1M','1M','1M','1M','1M','1M',
                '1M','1M','1M','1M','1M','1M','1M','1M']               
# 시청가능 디바이스(DEVICE) : M(모바일), T(TV), P(PC), MP(모바일+PC), MPT(모바일+PC+TV)
device_list = ['MP','M','MP','MPT','M','MPT','MP','MP','MP','MP',
               'MPT','MP','MPT','MP','MPT','MP','MPT','MP','MPT','MP',
               'MPT','MP','MP','MPT','MP','MP','MP','MPT','M','MPT',
               'MPT','M','MPT','MPT','M','MPT','MPT','M','MPT','MPT',
               'MPT','MPT','MPT','MPT','MPT','MP','MP','MPT','MPT','MP',
               'MP','MPT','MPT','MPT','MP','MP','MPT','MP','MP','MP',
               'MPT','MPT','MP','MP','MP','MPT','MPT','MP','MP','T',
               'MP','MP','M','MPT','M','MPT','MPT','MPT','MPT','MPT',
               'MPT','MPT','MPT','MPT','MPT','MPT','M','MPT']
# 시청권한(TYPE) : L(LIVE), V(VOD), LV(LIVE+VOD), LVP(LIVE+VOD+PLAYY)
type_list = ['LV','LV','LV','LV','LV','LV','LV','LV','LVP','LV',
             'LV','LV','LV','LV','LV','LV','LV','LV','LV','LV',
             'LV','LVP','LVP','LVP','LVP','LVP','LVP','LVP','V','V',
             'V','LV','LV','LV','LV','LV','V','LV','L','L',
             'V','V','LV','LV','LV','LV','LV','LV','LV','LV',
             'LVP','LVP','LV','LV','LV','LVP','LVP','L','V','LV',
             'LV','LV','LV','LV','LV','LV','LV','LV','LV','V',
             'L','LV','LV','LV','LVP','LVP','LVP','LVP','LVP','LV',
             'LV','LV','LV','LVP','LVP','LVP','LV','LV']
# 화질(IMAGE) : HD(HD), FHD(FHD), UHD(UHD)
iamge_list = ['HD','HD','HD','FHD','HD','FHD','HD','HD','HD','HD',
              'FHD','HD','FHD','HD','FHD','HD','FHD','HD','FHD','HD',
              'FHD','HD','HD','FHD','HD','HD','HD','FHD','HD','FHD',
              'UHD','HD','FHD','FHD','HD','FHD','HD','HD','FHD','UHD',
              'FHD','UHD','FHD','UHD','FHD','HD','HD','FHD','UHD','HD',
              'HD','FHD','FHD','UHD','HD','HD','FHD','HD','HD','HD',
              'FHD','UHD','HD','HD','HD','FHD','UHD','HD','HD','FHD',
              'HD','HD','HD','FHD','HD','FHD','FHD','FHD','UHD','FHD',
              'FHD','FHD','FHD','FHD','FHD','FHD','HD','FHD']
# VOD다운로드(VODDOWN) : V0(VOD다운불가), V10(VOD10회), V20(VOD20회), VINF(VOD무제한)
voddown_list = ['V10','V10','V10','V10','V10','V10','VINF','VINF','V10','V10',
                'V10','V10','V10','V10','V10','VINF','VINF','VINF','VINF','VINF',
                'VINF','V10','V10','V10','V10','VINF','VINF','VINF','V0','V0',
                'V0','V10','V10','V10','VINF','VINF','V0','V10','V0','V0',
                'V0','V0','V10','V10','VINF','V10','V0','V0','V0','V0',
                'V0','V0','V0','V0','V0','V0','V0','V0','V0','V0',
                'V0','V0','V0','V0','V0','V0','V0','V0','V0','V0',
                'V0','V0','V10','V0','V10','V10','V10','V10','V10','V10',
                'V10','V30','V30','V10','V10','VINF','V10','V10']
# 결합상품(COMBINED) : Y(KB,BUGS,FLO), N
combined_list = ['N','N','N','N','N','N','N','N','N','N',
                 'N','N','N','N','N','N','N','N','N','N',
                 'N','N','N','N','N','N','N','N','N','N',
                 'N','N','N','N','N','N','N','Y','N','N',
                 'N','N','N','N','N','N','N','N','N','Y',
                 'N','N','N','N','N','N','N','N','N','N',
                 'N','N','Y','Y','N','N','N','Y','Y','N',
                 'N','N','N','N','N','N','N','N','N','N',
                 'N','N','N','N','N','N','N','N']
# STORE(STORE) : NO(None), AS(iTunes), OS(One Store)
store_list = ['NO','NO','NO','NO','NO','NO','NO','NO','NO','OS',
              'AS','AS','AS','AS','AS','AS','AS','AS','AS','AS',
              'AS','AS','AS','AS','AS','AS','AS','AS','NO','NO',
              'NO','NO','NO','NO','NO','NO','NO','NO','AS','AS',
              'AS','AS','AS','AS','AS','NO','NO','NO','NO','NO',
              'NO','NO','AS','AS','AS','AS','AS','NO','NO','NO',
              'NO','NO','NO','NO','OS','OS','OS','OS','OS','NO',
              'NO','NO','AS','AS','NO','NO','NO','AS','AS','AS',
              'AS','AS','AS','AS','AS','AS','NO','NO']

pcode_struct = {'productcode': idx_pcode,
                'PC_PAYMENT': payment_list,
                'PC_DEVICE': device_list,
                'PC_TYPE': type_list,
                'PC_IMAGE': iamge_list,
                'PC_VODDOWN': voddown_list,
                'PC_COMBINED': combined_list,
                'PC_STORE': store_list}

df_productcode = pd.DataFrame(pcode_struct)
df_productcode

df_productcode.info()

for column in df_productcode.columns.values.tolist():
    print(column)
    print(df_productcode[column].unique())
    print("")

# 변수 생성(Feature Extraction) (예시)
#   (1) 고객별 서비스 가입 이력 수
#   (2) 고객별 서비스 가입 이력 상품 수
#   (3) 고객별 시청 건수 (1시간 단위)
#   (4) 고객별 시청 총 시간
#   (5) 고객별 시청 평균 시간
#   (6) 고객별 시청 채널 수
#   (7) 고객별 시청 프로그램 수
#   (8) 고객별 시청 기기 대수
#   (9) 3주차 시청 타임 산출
#   (10) 3주차 시청시간 비율 = 3주차 시청시간 / 전체 시청시간
#   (11) 고객별 실결제 금액의 합(Sum) > normalization
#   (12) 고객별 실결제 금액의 평균(Mean) > normalization
#   (13) 고객별 실결제 금액의 표준편차(STD) > normalization
################## 아래는 적용 전 ####################
#   (14) service > productcode 별 시청가능 device 개수
#   (15) service > productcode 별 자동결제/이용권/30일/월정액가입자/다운로드/결합상품(벅스,다음TV,멜론,PLAYY영화)/iTunes 구분

# (1) 고객별 서비스 가입 이력 수 - OK - Normalization
df_feature_1 = df_service.groupby(by='uno', as_index=False).registerdate.count()
df_feature_1.rename(columns={'registerdate':'REG_CNT'}, inplace=True)
print('REG_CNT = ', sorted(df_feature_1.REG_CNT.unique().tolist()))
#print('REG_CNT = ', df_feature_1.REG_CNT.value_counts())

# (2) 고객별 서비스 가입 이력 상품 수 - OK - Normalization
df_feature_2 = df_service[['uno','productcode']]
df_feature_2 = df_feature_2.drop_duplicates() # 고객별 동일 상품 제거
df_feature_2 = df_feature_2.groupby(by='uno', as_index=False).productcode.count()
df_feature_2.rename(columns={'productcode':'PRD_CNT'}, inplace=True)
print('PRD_CNT = ', sorted(df_feature_2.PRD_CNT.unique().tolist()))
#print('PRD_CNT = ', df_feature_2.PRD_CNT.value_counts())

# (3) 고객별 시청 건수 - OK - Normalization
df_feature_3 = df_bookmark.groupby(by='uno', as_index=False).dates.count()
df_feature_3.rename(columns={'dates':'BM_CNT'}, inplace=True)
print('BM_CNT = ', sorted(df_feature_3.BM_CNT.unique().tolist()))
#print('BM_CNT = ', df_feature_3.BM_CNT.value_counts())

# (4) 고객별 시청 총 시간 - OK - Normalization
df_feature_4 = df_bookmark.groupby(by='uno', as_index=False).viewtime.sum()
df_feature_4.rename(columns={'viewtime':'VT_TOT'}, inplace=True)
print('VT_TOT = ', sorted(df_feature_4.VT_TOT.unique().tolist()))
#print('VT_TOT = ', df_feature_4.VT_TOT.value_counts())

# (5) 고객별 시청 평균 시간 - OK - Normalization
df_feature_5 = df_bookmark.groupby(by='uno', as_index=False).viewtime.mean()
df_feature_5.rename(columns={'viewtime':'VT_AVG'}, inplace=True)
print('VT_AVG = ', sorted(df_feature_5.VT_AVG.unique().tolist()))
#print('VT_AVG = ', df_feature_5.VT_AVG.value_counts())

# (6) 고객별 시청 콘텐츠 종류 수 - OK - Normalization
df_feature_6 = df_bookmark[['uno','channeltype']]
df_feature_6 = df_feature_6.drop_duplicates() # 고객별 동일 채널 제거
df_feature_6 = df_feature_6.groupby(by='uno', as_index=False).channeltype.count()
df_feature_6.rename(columns={'channeltype':'CH_CNT'}, inplace=True)
print('CH_CNT = ', sorted(df_feature_6.CH_CNT.unique().tolist()))
#print('CH_CNT = ', df_feature_6.CH_CNT.value_counts())

# (7) 고객별 시청 프로그램 수 - OK - Normalization
df_feature_7 = df_bookmark[['uno','programid']]
df_feature_7 = df_feature_7.drop_duplicates() # 고객별 동일 프로그램 제거
df_feature_7 = df_feature_7.groupby(by='uno', as_index=False).programid.count()
df_feature_7.rename(columns={'programid':'PRG_CNT'}, inplace=True)
print('PRG_CNT = ', sorted(df_feature_7.PRG_CNT.unique().tolist()))
#print('PRG_CNT = ', df_feature_7.PRG_CNT.value_counts())

# (8) 고객별 시청 기기 대수 - OK - Normalization
df_feature_8 = df_bookmark[['uno','devicetype']]
df_feature_8 = df_feature_8.drop_duplicates() # 고객별 동일 프로그램 제거
df_feature_8 = df_feature_8.groupby(by='uno', as_index=False).devicetype.count()
df_feature_8.rename(columns={'devicetype':'DEV_CNT'}, inplace=True)
print('DEV_CNT = ', sorted(df_feature_8.DEV_CNT.unique().tolist()))
#print('DEV_CNT = ', df_feature_8.DEV_CNT.value_counts())

# 2021.07.05 by DH
# (9) 고객별 coin 결제 종류
df_feature_9 = df_coin[['uno','paymenttypeid']]
df_feature_9 = df_feature_9.drop_duplicates() # 고객별 동일 프로그램 제거
df_feature_9 = df_feature_9.groupby(by='uno', as_index=False).paymenttypeid.count()
df_feature_9.rename(columns={'paymenttypeid':'CON_CNT'}, inplace=True)
print('CON_CNT = ', sorted(df_feature_9.CON_CNT.unique().tolist()))
#print('CON_CNT = ', df_feature_9.CON_CNT.value_counts())

# (10) 고객별 실결제 금액의 합(Sum) - OK - Normalization
df_feature_10 = df_service[['uno','pgamount']]
df_feature_10 = df_feature_10.groupby(by='uno', as_index=False).pgamount.sum()
df_feature_10.rename(columns={'pgamount':'PGA_SUM'}, inplace=True)
print('PGA_SUM = ', sorted(df_feature_10.PGA_SUM.unique().tolist()))

# (11) 고객별 실결제 금액의 평균(Mean) - OK - Normalization
df_feature_11 = df_service[['uno','pgamount']]
df_feature_11 = df_feature_11.groupby(by='uno', as_index=False).pgamount.mean()
df_feature_11.rename(columns={'pgamount':'PGA_MEAN'}, inplace=True)
print('PGA_MEAN = ', sorted(df_feature_11.PGA_MEAN.unique().tolist()))

# (12) 고객별 실결제 금액의 표준편차(STD) - OK - Normalization
df_feature_12 = df_service[['uno','pgamount']]
df_feature_12 = df_feature_12.groupby(by='uno', as_index=False).pgamount.std()
df_feature_12.rename(columns={'pgamount':'PGA_STD'}, inplace=True)
print('PGA_STD = ', sorted(df_feature_12.PGA_STD.unique().tolist()))

# 개인별 / 일자별 시청 시간 분석
# 예측 시점인 3주차에도 많이 보는지 여부 확인
date_viewtiime_byuno = df_bookmark.pivot_table(index="uno",
                                               columns="dates", 
                                               values="viewtime", 
                                               aggfunc="sum", 
                                               fill_value=0)

# get 'registerdate' from df_service
service_date_viewtime = pd.DataFrame()
service_date_viewtime = pd.merge(date_viewtiime_byuno, df_svc_target[['uno', 'registerdate']], on='uno', how='left')

# 3주차 시청 타임 산출
import datetime
from dateutil.relativedelta import relativedelta

service_date_viewtime['3rdweek_vt'] = 0
max_dates = df_bookmark.dates.max()

for i in range(len(service_date_viewtime)):
    starttime = service_date_viewtime.loc[i, 'registerdate'] + datetime.timedelta(weeks=2)
    endtime = starttime + datetime.timedelta(weeks=1)

    if endtime > max_dates:
        endtime = max_dates

    start_year = int(starttime.strftime('%Y'))
    start_month = int(starttime.strftime('%m'))
    start_day = int(starttime.strftime('%d'))

    end_year = int(endtime.strftime('%Y'))
    end_month = int(endtime.strftime('%m'))
    end_day = int(endtime.strftime('%d'))
    
    service_date_viewtime['3rdweek_vt'].iloc[i] = service_date_viewtime.loc[i, datetime.datetime(start_year, start_month, start_day):datetime.datetime(end_year, end_month, end_day)].sum()

# 3주차 시청시간 비율 = 3주차 시청시간 / 전체 시청시간
service_date_viewtime['3rdweek_ratio'] = service_date_viewtime['3rdweek_vt'] / df_feature_4['VT_TOT'] * 100

# From train_service.csv
print('df_feature_1 = ', df_feature_1.shape[0])
print('df_feature_2 = ', df_feature_2.shape[0])
# From train_bookmark.csv
print('df_feature_3 = ', df_feature_3.shape[0])
print('df_feature_4 = ', df_feature_4.shape[0])
print('df_feature_5 = ', df_feature_5.shape[0])
print('df_feature_6 = ', df_feature_6.shape[0])
print('df_feature_7 = ', df_feature_7.shape[0])
print('df_feature_8 = ', df_feature_8.shape[0])
# From coin.csv
print('df_feature_9 = ', df_feature_9.shape[0]) # 2021.07.05 by DH
# 3week
print('3rdweek_vt = ', service_date_viewtime.shape[0]) # 07.14 
print('3rdweek_ratio = ', service_date_viewtime.shape[0]) # 07.14 
print('df_feature_10 = ', df_feature_10.shape[0]) # 2021.07.21 by DH
print('df_feature_11 = ', df_feature_11.shape[0]) # 2021.07.21 by DH
print('df_feature_12 = ', df_feature_12.shape[0]) # 2021.07.21 by DH

# 해지 예측 대상 서비스에 생성한 변수 연결
df_svc_target = pd.merge(df_svc_target, df_feature_1, on='uno', how='left')
df_svc_target = pd.merge(df_svc_target, df_feature_2, on='uno', how='left')
df_svc_target = pd.merge(df_svc_target, df_feature_3, on='uno', how='left')
df_svc_target = pd.merge(df_svc_target, df_feature_4, on='uno', how='left')
df_svc_target = pd.merge(df_svc_target, df_feature_5, on='uno', how='left')
df_svc_target = pd.merge(df_svc_target, df_feature_6, on='uno', how='left')
df_svc_target = pd.merge(df_svc_target, df_feature_7, on='uno', how='left')
df_svc_target = pd.merge(df_svc_target, df_feature_8, on='uno', how='left')
df_svc_target = pd.merge(df_svc_target, df_feature_9, on='uno', how='left') # 2021.07.05 by DH
df_svc_target = pd.merge(df_svc_target, df_feature_10, on='uno', how='left') # 2021.07.21
df_svc_target = pd.merge(df_svc_target, df_feature_11, on='uno', how='left') # 2021.07.21
df_svc_target = pd.merge(df_svc_target, df_feature_12, on='uno', how='left') # 2021.07.21
df_svc_target = pd.merge(df_svc_target, service_date_viewtime[['uno', '3rdweek_vt', '3rdweek_ratio']], on='uno', how='left') # 07.14
df_svc_target = pd.merge(df_svc_target, df_productcode, on='productcode', how='left') # 07.24

df_svc_target.info()

# 추가 생성 컬럼들의 결측치 처리
df_svc_target = df_svc_target.fillna(-999) #(0)
df_svc_target = df_svc_target.astype({'concurrentwatchcount':'float64', 'REG_CNT':'float64', 'PRD_CNT':'float64'}) # 2021.07.05 by DH

# skewness 비대칭을 줄이기 위해 np.log 적용
df_svc_target['pgamount']= df_svc_target['pgamount'].map(lambda i : np.log(i) if i > 0 else 0)
df_svc_target['BM_CNT']= df_svc_target['BM_CNT'].map(lambda i : np.log(i) if i > 0 else 0)
df_svc_target['PRG_CNT']= df_svc_target['PRG_CNT'].map(lambda i : np.log(i) if i > 0 else 0)
df_svc_target['3rdweek_vt']= df_svc_target['3rdweek_vt'].map(lambda i : np.log(i) if i > 0 else 0)

# 최종 분석 대상 데이터 : df_T
df_T = df_svc_target.copy()
df_T.info()
df_T.sample(5)

"""> ### 3) Label Encoding"""

# Encoding categorical variables to numeric ones

# Label 컬럼은 직접 변환
print('Repurchase = ', df_T.Repurchase.unique().tolist())
print(df_T.Repurchase.value_counts())
# 재결제('O') -> Churn Negative(0), 미결제('X') -> Churn Positive(1)
df_T['CHURN'] = np.where(df_T.Repurchase == 'X', 1, 0)   
print('CHURN = ', df_T.CHURN.unique().tolist())
print(df_T.CHURN.value_counts())

# 이외 컬럼들은 일괄 자동 변환
col_lst = df_T.columns.values.tolist()
col_lst.remove('uno')
col_lst.remove('registerdate')
col_lst.remove('enddate')
col_lst.remove('productcode')
col_cat_lst = []

for c in col_lst:
    if (df_T[c].dtype == 'object' or df_T[c].dtype == 'int64'): # int64를 string으로 바꾸고 해야하나?
        print(c+'_label encode')
        # category list 만들기
        col_cat_lst.append(c)
        #print('col_cat_lst =', col_cat_lst)
        df_T[c],_ = df_T[c].factorize()
    if df_T[c].dtype == 'float64':
        print(c+'_normalize')
        df_T[c] = (df_T[c]-df_T[c].mean())/df_T[c].std()

df_T.info()

# 기술 통계 현황 확인
df_T.describe()

df_T

"""> ### 4) 상관관계/분포도 확인"""

# Commented out IPython magic to ensure Python compatibility.
# Basic correlation plot to understand which features are correlated
import matplotlib.pyplot as plt
import seaborn as sns

# %matplotlib inline

plt.figure(figsize=(18,10))
sns.heatmap(df_T.corr(), annot=True)

# This also helps in finding number of counts in each column
df_T.hist(figsize=(20,16))
plt.show()  # showing the charts of different columns

# 2021.07.05 by DH
# Draw a Pairplot
#plt.figure(figsize=(18,10))
#sns.pairplot(df_T)
#plt.show()

"""## **4. 모델 훈련**

> ### 1) Train/Test 데이터 설정
"""

from sklearn.model_selection import train_test_split

# random_state 고정
SEED = 42

# Train/Test 데이터 분리 
X = df_T.drop(["uno","registerdate","enddate","productcode","Repurchase","CHURN"], axis=1)
y = df_T["CHURN"]

#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = SEED) # 07.21일 7:3으로 시도 해 볼까?
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = SEED) # 07.21일 7:3으로 시도 해 볼까?
# Train category column 정리
col_cat_lst.remove('Repurchase')
col_cat_lst.remove('CHURN')

print(col_cat_lst)

print('Traing X     = ', len(X_train), ', Training Y   = ', len(y_train))
print('Validation X = ', len(X_test),  ', Validation Y = ', len(y_test))

# 모델별 성능 확인을 위한 함수
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import mean_squared_error

my_predictions = {}
my_pred = None
my_actual = None
my_name = None

colors = ['r', 'c', 'm', 'y', 'k', 'khaki', 'teal', 'orchid', 'sandybrown',
          'greenyellow', 'dodgerblue', 'deepskyblue', 'rosybrown', 'firebrick',
          'deeppink', 'crimson', 'salmon', 'darkred', 'olivedrab', 'olive', 
          'forestgreen', 'royalblue', 'indigo', 'navy', 'mediumpurple', 'chocolate',
          'gold', 'darkorange', 'seagreen', 'turquoise', 'steelblue', 'slategray', 
          'peru', 'midnightblue', 'slateblue', 'dimgray', 'cadetblue', 'tomato'
         ]

def plot_predictions(name_, pred, actual):
    df = pd.DataFrame({'prediction': pred, 'actual': actual})
    df = df.sort_values(by='actual').reset_index(drop=True)

    plt.figure(figsize=(8, 5))
    plt.scatter(df.index, df['prediction'], marker='x', color='r')
    plt.scatter(df.index, df['actual'], alpha=0.7, marker='o', color='blue')
    plt.title(name_, fontsize=15)
    plt.legend(['prediction', 'actual'], fontsize=12)
    plt.show()

def f1_eval(name_, pred, actual):
    global my_predictions, colors, my_pred, my_actual, my_name
    
    my_name = name_
    my_pred = pred
    my_actual = actual
    
    print('accuracy  = ', accuracy_score(pred, actual))
    print('precision = ', precision_score(pred, actual))
    print('recall    = ', recall_score(pred, actual))
    print('f1        = ', f1_score(pred, actual))
    print("")
    print(classification_report(pred, actual))
    print(confusion_matrix(pred, actual))

#    plot_predictions(name_, pred, actual)

    f1s = f1_score(pred, actual)
    my_predictions[name_] = f1s

    y_value = sorted(my_predictions.items(), key=lambda x: x[1], reverse=False)
    
    df = pd.DataFrame(y_value, columns=['model', 'f1'])
    print(df)
#    min_ = df['f1'].min() - 1
#    max_ = df['f1'].max() + 1
    
    length = len(df) / 2
    
    plt.figure(figsize=(9, length))
    ax = plt.subplot()
    ax.set_yticks(np.arange(len(df)))
    ax.set_yticklabels(df['model'], fontsize=12)
    bars = ax.barh(np.arange(len(df)), df['f1'], height=0.8)
    
    for i, v in enumerate(df['f1']):
        idx = np.random.choice(len(colors))
        bars[i].set_color(colors[idx])
        ax.text(v + 0.05, i, str(round(v, 3)), color='k', fontsize=12, fontweight='bold', verticalalignment='center')
        
    plt.title('f1 score', fontsize=16)
#    plt.xlim(min_, max_)
    plt.xlim(left=0.0, right=1.0)
    
    plt.show()

"""> ### 2) 모델 정의 및 학습(모델별)

> #### 3-1) LogisticRegression - 모델 생성
"""

if (b_lr_active or b_voting_active):
    from sklearn.linear_model import LogisticRegression

    lr = LogisticRegression(solver='lbfgs', penalty='l2', C=1, max_iter=100, random_state=SEED)
    lr.fit(X_train, y_train)
    y_pred = lr.predict(X_test)

    f1_eval('LogisticRegression', y_pred, y_test)

"""> #### 3-2) Decision Tree - 모델 생성 """

if (b_tree_active or b_voting_active):
    from sklearn.tree import DecisionTreeClassifier

    tree = DecisionTreeClassifier(criterion='gini', max_depth=None, max_leaf_nodes=None,
                                min_samples_split=2, min_samples_leaf=1, max_features=None,
                                random_state=SEED)
    tree.fit(X_train, y_train)
    y_pred = tree.predict(X_test)

    f1_eval('DecisionTreeClassifier', y_pred, y_test)

"""> #### 3-3) Voting - 모델 생성 """

if b_voting_active:
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.linear_model import LogisticRegression, RidgeClassifier
    from sklearn.ensemble import VotingClassifier

    #lr = LogisticRegression(random_state=SEED)
    knn = KNeighborsClassifier(n_neighbors=8)

    single_models = [
        ('logistic_reg', lr), 
        ('knn_class', knn), 
        ('decision_tree', tree)
    ]
    voting = VotingClassifier(single_models, voting='soft', n_jobs=-1)
    voting.fit(X_train, y_train)
    y_pred = voting.predict(X_test)

    f1_eval('VotingClassifier', y_pred, y_test)

"""> #### 3-4) Bagging : Random Forest - 모델 생성 """

if (b_rfc_active or b_stack_active):
    from sklearn.ensemble import RandomForestClassifier

    rfc = RandomForestClassifier(random_state=SEED)
    #rfc = RandomForestClassifier(n_estimators=1000, max_depth=5, random_state=SEED)
    rfc.fit(X_train, y_train)
    y_pred = rfc.predict(X_test)

    f1_eval('RandomForestClassifier', y_pred, y_test)

"""> #### 3-5) GBM - 모델 생성 """

if b_gbc_active:
    from sklearn.ensemble import GradientBoostingClassifier

    gbc = GradientBoostingClassifier(random_state=SEED) #default learning_rate=0.1 / n_estimators=100
    #gbc = GradientBoostingClassifier(learning_rate=0.01, n_estimators=4500, subsample=0.8, random_state=SEED) #최적? 0.1, 450
    gbc.fit(X_train, y_train)
    y_pred = gbc.predict(X_test)

    f1_eval('GradientBoostingClassifier', y_pred, y_test)

"""> #### 3-6) XGBoost - 모델 생성

1. Xgboost 장점
Xgboost는 기존 GBDT 모델에 비해서 다음 기능이 있다.


*   정규화(Regularization)
*   병렬 처리
*   고수준의 유연성
*   결측치 처리
*   Tree Pruning
*   내장 Cross Validation
*   기존 모델에 이어서 재학습할 수 있음
"""

if (b_xgb_active or b_stack_active):
    from xgboost import XGBClassifier

# Default model
#    xgb = XGBClassifier()
   
    xgb = XGBClassifier(learning_rate=0.1, max_depth=8, n_estimators=2650,
                        eval_metric='aucpr', colsample_bytree=0.8, subsample=0.8,
                        use_label_encoder=False, min_child_weight=4, n_jobs=-1,
                        booster='gbtree', importance_type='gain',
                        objective = 'binary:logistic', random_state=SEED)

    xgb.fit(X_train, y_train)
    y_pred = xgb.predict(X_test)

    f1_eval('XGBClassifier w/tuning', y_pred, y_test)

if (b_xgb_active or b_stack_active):
    # xgb feature importance
    xgb.feature_importances_

if (b_xgb_active or b_stack_active):
    # plot xgboost feature importances
    import xgboost
    fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(20, 8))

    axes = [ax for row_axes in axes for ax in row_axes]
    xgboost.plot_importance(xgb, importance_type='gain', title='gain', xlabel='', grid=False, ax=axes[0])
    xgboost.plot_importance(xgb, importance_type='cover', title='cover', xlabel='', grid=False, ax=axes[1])
    xgboost.plot_importance(xgb, importance_type='weight', title='weight', xlabel='', grid=False, ax=axes[2])
    xgboost.plot_importance(xgb, importance_type='total_gain', title='total_gain', xlabel='', grid=False, ax=axes[3])
    xgboost.plot_importance(xgb, importance_type='total_cover', title='total_cover', xlabel='', grid=False, ax=axes[4])

    plt.tight_layout()
    plt.show()

"""> #### 3-7) LGBM - 모델 생성"""

if (b_lgbm_active or b_stack_active):
    from lightgbm import LGBMClassifier
    # from GridSearchCV, 결과값 메모장 참조
#    lgbm = LGBMClassifier(n_estimators=1100, learning_rate=0.01, max_depth=3, 
#                        colsample_bytree=0.8, subsample=0.8, num_leaves=10,
#                        objective='binary', metric='binary_logloss', boosting_type='gbdt',
#                        n_jobs=-1, random_state=SEED, importance_type='gain')
    # from Bayesian + GridSearchCV
    lgbm = LGBMClassifier(objective='binary',
                          metric='binary_logloss',
                          boosting_type='gbdt',
                          n_estimators=99,
                          learning_rate=0.1,
                          max_depth=67, #22,
                          subsample=0.7921, #0.8928,
                          subsample_freq=3,
                          num_leaves=35, #10,
                          colsample_bytree=0.4591, #0.6375,
                          reg_alpha=0,
                          reg_lambda=2,
                          min_split_gain=0,
                          min_child_samples=10,
                          min_child_weight=67.47, #6.582,
                          scale_pos_weight=7.13, #2.751,
                          n_jobs=-1,
                          random_state=SEED,
                          importance_type='gain')
        
    lgbm.fit(X_train, y_train, verbose=1, categorical_feature=col_cat_lst)
    y_pred = lgbm.predict(X_test)

    f1_eval('LGBMClassifier w/tuning', y_pred, y_test)

if (b_lgbm_active or b_stack_active):
    # xgb feature importance
    lgbm.feature_importances_

if (b_lgbm_active or b_stack_active):
    # plot lgbm feature importances
    import lightgbm
    fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(20, 8))

    axes = [ax for row_axes in axes for ax in row_axes]
    lightgbm.plot_importance(lgbm, importance_type='gain', title='gain', xlabel='', grid=False, ax=axes[0])

    plt.tight_layout()
    plt.show()

"""> #### 3-8) Stacking - 모델 생성"""

if b_stack_active:
    from sklearn.ensemble import StackingClassifier

    stack_models = [('randomforest', rfc),
                    ('lgbm', lgbm),
                    ]

    stack = StackingClassifier(stack_models, final_estimator=xgb, n_jobs=-1)
    stack.fit(X_train, y_train)
    y_pred = stack.predict(X_test)

    f1_eval('StackingClassifier', y_pred, y_test)

"""> ### [참조1.] RandomizedSearchCV

*   https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html?highlight=randomizedsearchcv

> #### (1) LGBMClassifier
"""

if b_randomSCV_lgbm:
    # Example1 : LGBMClassifier - RandomizedSearchCV
    params = {
        'n_estimators': [6200], 
        'learning_rate': [0.1], 
        'max_depth': [8], 
        'colsample_bytree': [0.8], 
        'subsample': [0.8],
    }
    from lightgbm import LGBMClassifier
    from sklearn.model_selection import RandomizedSearchCV

    lgbm_rscv = LGBMClassifier(n_jobs=-1, random_state=SEED)

    rand_search = RandomizedSearchCV(lgbm_rscv, params, cv=5, n_iter=25 , n_jobs=-1, scoring='f1')
    rand_search.fit(X_train, y_train)

if b_randomSCV_lgbm:
    rand_search.best_score_

if b_randomSCV_lgbm:
    rand_search.best_params_

if b_randomSCV_lgbm:
    lgbm_RSCV = LGBMClassifier(**rand_search.best_params_)
    lgbm_RSCV.fit(X_train, y_train)
    y_pred = lgbm_RSCV.predict(X_test)

    f1_eval('LGBMClassifier_RSCV', y_pred, y_test)

"""> #### (2) XGBoost """

if b_randomSCV_xgb:
    # Example2 : XGBClassifier - RandomizedSearchCV
    params = {
        'n_estimators': [1500, 1650, 1800], 
        'learning_rate': [0.1, 0.05, 0.01], 
        'max_depth': [3, 5, 7, 8], 
        'colsample_bytree': [0.8, 0.9], 
        'subsample': [0.8, 0.9,],
        'min_child_weight': [1, 2, 3, 4]
    }
    from xgboost import XGBClassifier
    from sklearn.model_selection import RandomizedSearchCV

    xgb_rscv = XGBClassifier(eval_metric = 'aucpr', use_label_encoder=False, objective = 'binary:logistic', random_state = SEED)

    rand_search = RandomizedSearchCV(xgb_rscv, params, cv=5, n_iter=25 , n_jobs=-1, scoring='f1')
    rand_search.fit(X_train, y_train)

if b_randomSCV_xgb:
    rand_search.best_score_

if b_randomSCV_xgb:
    rand_search.best_params_

if b_randomSCV_xgb:
    xgb_RSCV = XGBClassifier(**rand_search.best_params_)
    xgb_RSCV.fit(X_train, y_train)
    y_pred = xgb_RSCV.predict(X_test)

    f1_eval('XGBClassifier_RSCV', y_pred, y_test)

"""> ### [참조2.] GridSearchCV

*   https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearchcv#sklearn.model_selection.GridSearchCV
*   https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter

> #### (1) LGBMClassifier
"""

X_train.info()

if b_gridSCV_lgbm:
    # Example3 : LGBMClassifier - GridSearchCV
    params = {
        'n_estimators': [900, 999, 1100, 1200], #default=100
        'learning_rate': [0.01], #default=0.1
        'max_depth': [89], #[22], #default=-1
        'colsample_bytree': [0.5764], #[0.6375], #default=1
        'subsample': [0.726], #[0.8928], #default=1
        'num_leaves': [28], #[88], #< 2^(max_depth)
        'reg_alpha': [0],
        'reg_lambda': [2],
        'min_split_gain': [0],
        'min_child_samples': [10],
        'subsample_freq': [3],
        'scale_pos_weight': [4.078], #[2.751],
        'min_child_weight': [25.63]  #[6.582]
    }
    from lightgbm import LGBMClassifier
    from sklearn.model_selection import GridSearchCV

    lgbm_gscv = LGBMClassifier(objective='binary', metric='binary_logloss', 
                               boosting_type='gbdt', n_jobs=-1, random_state=SEED)

    grid_search = GridSearchCV(lgbm_gscv, params, cv=5, n_jobs=-1, scoring='f1')
    grid_search.fit(X_train, y_train, verbose=1, categorical_feature=col_cat_lst)

if b_gridSCV_lgbm:
    grid_search.best_score_

if b_gridSCV_lgbm:
    grid_search.best_params_

if b_gridSCV_lgbm:
    lgbm_GSCV = LGBMClassifier(**grid_search.best_params_)
    lgbm_GSCV.fit(X_train, y_train)
    y_pred = lgbm_GSCV.predict(X_test)

    f1_eval('LGBMClassifier_GSCV', y_pred, y_test)

"""> #### (2) XGBoost """

if b_gridSCV_xgb:
    # Example4 : XGBClassifier - GridSearchCV
    params = {
        'n_estimators': [2650], 
        'learning_rate': [0.1], 
        'max_depth': [8], 
        'colsample_bytree': [0.8], 
        'subsample': [0.8],
        'min_child_weight': [4]
    }
    from xgboost import XGBClassifier
    from sklearn.model_selection import GridSearchCV

    xgb_gscv = XGBClassifier(eval_metric = 'aucpr', use_label_encoder=False, objective = 'binary:logistic', random_state = SEED)

    grid_search = GridSearchCV(xgb_gscv, params, cv=5, n_jobs=-1, scoring='f1')
    grid_search.fit(X_train, y_train)

if b_gridSCV_xgb:
    grid_search.best_score_

if b_gridSCV_xgb:
    grid_search.best_params_

if b_gridSCV_xgb:
    xgb_GSCV = XGBClassifier(**grid_search.best_params_)
    xgb_GSCV.fit(X_train, y_train)
    y_pred = xgb_GSCV.predict(X_test)

    f1_eval('XGBClassifier_GSCV', y_pred, y_test)

"""> ### [참조3.] Bayesian Optimization

> #### (1) LGBMClassifier
"""

if b_bayesianOpt:
    !pip install bayesian-optimization
    import lightgbm as lgb
    from bayes_opt import BayesianOptimization
    from sklearn.model_selection import cross_val_score
    from sklearn.model_selection import train_test_split
    from numpy import loadtxt
    from sklearn.metrics import accuracy_score,confusion_matrix
    import numpy as np

    def lgb_evaluate(numLeaves, maxDepth, scaleWeight, minChildWeight, subsample, colSam):
        clf = lgb.LGBMClassifier(
            objective = 'binary',
            metric= 'binary_logloss',
            reg_alpha= 0,
            reg_lambda= 2,
            #bagging_fraction= 0.999,
            min_split_gain= 0,
            min_child_samples= 10,
            subsample_freq= 3,
            #subsample_for_bin= 50000,
            #n_estimators= 9999999,
            n_estimators= 99,
            learning_rate= 0.1,
            num_leaves= int(numLeaves),
            max_depth= int(maxDepth),
            scale_pos_weight= scaleWeight,
            min_child_weight= minChildWeight,
            subsample= subsample,
            colsample_bytree= colSam,
            verbose =-1)
        # metric : f1
        #scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='f1')
        # metric : roc_auc test on 7/26
        scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='roc_auc')
        return np.mean(scores)

    def bayesOpt(X_train, y_train):
        lgbBO = BayesianOptimization(lgb_evaluate, 
                                     {'numLeaves':(5,90), 'maxDepth':(2,90),
                                      'scaleWeight':(1,10000), 'minChildWeight':(0.01,70),
                                      'subsample':(0.4, 1), 'colSam':(0.4, 1)},
                                     random_state=SEED)
        lgbBO.maximize(init_points=5, n_iter=50)
        print(lgbBO.res)

    bayesOpt(X_train, y_train)

"""## **5. 예측 결과 파일 제출**

> ### 1) Predict 파일 로딩
"""

# Predict Input 파일 읽기 - 서비스 파일
ds_P_service = "predict_service.csv"
df_P_service = pd.read_csv(ds_P_service, parse_dates=['registerdate','enddate'], infer_datetime_format=True)

df_P_service.info()
#df_P_service.sample(5)

# Predict Input 파일 읽기 - 북마크 파일
ds_P_bookmark = "predict_bookmark.csv"
df_P_bookmark = pd.read_csv(ds_P_bookmark, parse_dates=['dates'], infer_datetime_format=True)

df_P_bookmark.info()
#df_pbook.sample(5)

# Coin 파일 읽기
ds_coin = "coin.csv"
df_coin = pd.read_csv(ds_coin, parse_dates=['registerdate'], infer_datetime_format=True)

# 데이터에 대한 전반적인 정보를 표시 dataframe을 구성하는 행과 열의 크기, 컬럼명, 컬럼을 구성하는 값의 자료형 등을 출력
df_coin.info()
#df_coin.sample(3)

"""> ### 2) 데이터 가공(학습 동일 적용)"""

# gender null 값을 N 으로 변경 후 확인
df_P_service['gender'] = df_P_service['gender'].fillna('N')

# 2021.07.05 by DH
# agegroup 120 이상 값을 0 으로 변환
df_P_service.loc[df_P_service['agegroup'] >= 120, 'agegroup'] = 0

# pgamount 금액 중에 달러로 결제된 것 원화로 변경 (pgamount 100원 미만인 건은 Appstore에서 달러 결제 건임)
df_P_service.loc[(df_P_service['pgamount'] <  100), 'pgamount'] = df_P_service['pgamount'] * 1120

# 기타 컬럼들의 결측치 처리
df_P_service = df_P_service.fillna('X')

# 해지 예측 대상 서비스 추출
# 즉, 예측시점(가입일+3주)에 해지하지 않은 서비스 추출
# 2021.07.06 by DH : Train 조건과 동일하게 맞춤
df_P_svc_target = df_P_service[df_P_service.enddate.dt.date > (df_P_service.registerdate + pd.DateOffset(weeks=3)).dt.date]

# 변수 생성(Feature Extraction) (예시)
# (1) 고객별 서비스 가입 이력 수
df_P_feature_1 = df_P_service.groupby(by='uno', as_index=False).registerdate.count()
df_P_feature_1.rename(columns={'registerdate':'REG_CNT'}, inplace=True)

# (2) 고객별 서비스 가입 이력 상품 수
df_P_feature_2 = df_P_service[['uno','productcode']]
df_P_feature_2 = df_P_feature_2.drop_duplicates() # 고객별 동일 상품 제거
df_P_feature_2 = df_P_feature_2.groupby(by='uno', as_index=False).productcode.count()
df_P_feature_2.rename(columns={'productcode':'PRD_CNT'}, inplace=True)

# (3) 고객별 시청 건수 (1시간 단위)
df_P_feature_3 = df_P_bookmark.groupby(by='uno', as_index=False).dates.count()
df_P_feature_3.rename(columns={'dates':'BM_CNT'}, inplace=True)

# (4) 고객별 시청 총 시간
df_P_feature_4 = df_P_bookmark.groupby(by='uno', as_index=False).viewtime.sum()
df_P_feature_4.rename(columns={'viewtime':'VT_TOT'}, inplace=True)

# (5) 고객별 시청 평균 시간
df_P_feature_5 = df_P_bookmark.groupby(by='uno', as_index=False).viewtime.mean()
df_P_feature_5.rename(columns={'viewtime':'VT_AVG'}, inplace=True)

# (6) 고객별 시청 채널 수
df_P_feature_6 = df_P_bookmark[['uno','channeltype']]
df_P_feature_6 = df_P_feature_6.drop_duplicates() # 고객별 동일 채널 제거
df_P_feature_6 = df_P_feature_6.groupby(by='uno', as_index=False).channeltype.count()
df_P_feature_6.rename(columns={'channeltype':'CH_CNT'}, inplace=True)

# (7) 고객별 시청 프로그램 수
df_P_feature_7 = df_P_bookmark[['uno','programid']]
df_P_feature_7 = df_P_feature_7.drop_duplicates() # 고객별 동일 프로그램 제거
df_P_feature_7 = df_P_feature_7.groupby(by='uno', as_index=False).programid.count()
df_P_feature_7.rename(columns={'programid':'PRG_CNT'}, inplace=True)

# (8) 고객별 시청 디바이스 수
df_P_feature_8 = df_P_bookmark[['uno','devicetype']]
df_P_feature_8 = df_P_feature_8.drop_duplicates() # 고객별 동일 프로그램 제거
df_P_feature_8 = df_P_feature_8.groupby(by='uno', as_index=False).devicetype.count()
df_P_feature_8.rename(columns={'devicetype':'DEV_CNT'}, inplace=True)

# (9) 고객별 coin 결제 종류 수 
df_P_feature_9 = df_coin[['uno','paymenttypeid']]
df_P_feature_9 = df_P_feature_9.drop_duplicates() # 고객별 동일 프로그램 제거
df_P_feature_9 = df_P_feature_9.groupby(by='uno', as_index=False).paymenttypeid.count()
df_P_feature_9.rename(columns={'paymenttypeid':'CON_CNT'}, inplace=True)

# (10) 고객별 실결제 금액의 평균(Mean) - OK - Normalization
df_P_feature_10 = df_P_service[['uno','pgamount']]
df_P_feature_10 = df_P_feature_10.groupby(by='uno', as_index=False).pgamount.sum()
df_P_feature_10.rename(columns={'pgamount':'PGA_SUM'}, inplace=True)

# (11) 고객별 실결제 금액의 평균(Mean) - OK - Normalization
df_P_feature_11 = df_P_service[['uno','pgamount']]
df_P_feature_11 = df_P_feature_11.groupby(by='uno', as_index=False).pgamount.mean()
df_P_feature_11.rename(columns={'pgamount':'PGA_MEAN'}, inplace=True)

# (12) 고객별 실결제 금액의 표준편차(STD) - OK - Normalization
df_P_feature_12 = df_P_service[['uno','pgamount']]
df_P_feature_12 = df_P_feature_12.groupby(by='uno', as_index=False).pgamount.std()
df_P_feature_12.rename(columns={'pgamount':'PGA_STD'}, inplace=True)

# 3rdweek feature
date_viewtiime_P_byuno = df_P_bookmark.pivot_table(index="uno", 
                                  columns="dates", 
                                  values="viewtime",
                                  aggfunc="sum", 
                                  fill_value=0)

# get 'registerdate' from df_service
service_date_viewtime_P = pd.merge(date_viewtiime_P_byuno, df_P_svc_target[['uno', 'registerdate']], on='uno', how='left')

# (10) 3주차 시청 타임 산출
import datetime
from dateutil.relativedelta import relativedelta

service_date_viewtime_P['3rdweek_vt'] = 0
max_dates = df_P_bookmark.dates.max()

for i in range(len(service_date_viewtime_P)):
    starttime = service_date_viewtime_P.loc[i, 'registerdate'] + datetime.timedelta(weeks=2)
    endtime = starttime + datetime.timedelta(weeks=1)

    if endtime > max_dates:
        endtime = max_dates

    start_year = int(starttime.strftime('%Y'))
    start_month = int(starttime.strftime('%m'))
    start_day = int(starttime.strftime('%d'))

    end_year = int(endtime.strftime('%Y'))
    end_month = int(endtime.strftime('%m'))
    end_day = int(endtime.strftime('%d'))

    service_date_viewtime_P['3rdweek_vt'].iloc[i] = service_date_viewtime_P.loc[i, datetime.datetime(start_year, start_month, start_day):datetime.datetime(end_year, end_month, end_day)].sum()

# (11) 3주차 시청시간 비율 = 3주차 시청시간 / 전체 시청시간
service_date_viewtime_P['3rdweek_ratio'] = service_date_viewtime_P['3rdweek_vt'] / df_P_feature_4['VT_TOT'] * 100

# 해지 예측 대상 서비스에 생성한 변수 연결
df_P_svc_target = pd.merge(df_P_svc_target, df_P_feature_1, on='uno', how='left')
df_P_svc_target = pd.merge(df_P_svc_target, df_P_feature_2, on='uno', how='left')
df_P_svc_target = pd.merge(df_P_svc_target, df_P_feature_3, on='uno', how='left')
df_P_svc_target = pd.merge(df_P_svc_target, df_P_feature_4, on='uno', how='left')
df_P_svc_target = pd.merge(df_P_svc_target, df_P_feature_5, on='uno', how='left')
df_P_svc_target = pd.merge(df_P_svc_target, df_P_feature_6, on='uno', how='left')
df_P_svc_target = pd.merge(df_P_svc_target, df_P_feature_7, on='uno', how='left')
df_P_svc_target = pd.merge(df_P_svc_target, df_P_feature_8, on='uno', how='left')
df_P_svc_target = pd.merge(df_P_svc_target, df_P_feature_9, on='uno', how='left') # 2021.07.05 by DH
df_P_svc_target = pd.merge(df_P_svc_target, df_P_feature_10, on='uno', how='left') # 2021.07.21
df_P_svc_target = pd.merge(df_P_svc_target, df_P_feature_11, on='uno', how='left') # 2021.07.21
df_P_svc_target = pd.merge(df_P_svc_target, df_P_feature_12, on='uno', how='left') # 2021.07.21
df_P_svc_target = pd.merge(df_P_svc_target, service_date_viewtime_P[['uno', '3rdweek_vt', '3rdweek_ratio']], on='uno', how='left') # 07.14
df_P_svc_target = pd.merge(df_P_svc_target, df_productcode, on='productcode', how='left') # 07.24

# 추가 생성 컬럼들의 결측치 처리
df_P_svc_target = df_P_svc_target.fillna(-999)

# type 변경
df_P_svc_target = df_P_svc_target.astype({'concurrentwatchcount':'float64', 'REG_CNT':'float64', 'PRD_CNT':'float64'})

# skewness 비대칭을 줄이기 위해 np.log 적용
df_P_svc_target['pgamount']= df_P_svc_target['pgamount'].map(lambda i : np.log(i) if i > 0 else 0)
df_P_svc_target['BM_CNT']= df_P_svc_target['BM_CNT'].map(lambda i : np.log(i) if i > 0 else 0)
df_P_svc_target['PRG_CNT']= df_P_svc_target['PRG_CNT'].map(lambda i : np.log(i) if i > 0 else 0)
df_P_svc_target['3rdweek_vt']= df_P_svc_target['3rdweek_vt'].map(lambda i : np.log(i) if i > 0 else 0)

df_P_svc_target.info()

"""> ### 3) Label Encoding"""

# 최종 분석 대상 데이터
df_P = df_P_svc_target.copy()

# Encoding categorical variables to numeric ones
col_lst = df_P.columns.values.tolist()
col_lst.remove('uno')
col_lst.remove('registerdate')
col_lst.remove('enddate')
col_lst.remove('productcode')

for c in col_lst:
    if (df_P[c].dtype == 'object' or df_P[c].dtype == 'int64'):
        print(c+'_label encode')
        df_P[c],_ = df_P[c].factorize()
    if df_P[c].dtype == 'float64':
        print(c+'_normalize')
        df_P[c] = (df_P[c]-df_P[c].mean())/df_T[c].std()

df_P.info()

# 예측 모델 입력을 위한 최종 데이터
X_predict = df_P.drop(["uno","registerdate","enddate","productcode","Repurchase"], axis=1)

X_predict.info()

"""> ### 4) 결과 예측"""

# 예측 모델에 최종 데이터 입력
if b_lr_active:
    y_pred = lr.predict(X_predict)

if b_tree_active:
    y_pred = tree.predict(X_predict)

if b_voting_active:
    y_pred = voting.predict(X_predict)

if b_rfc_active:
    y_pred = rfc.predict(X_predict)

if b_gbc_active:
    y_pred = gbc.predict(X_predict)

if b_xgb_active:
    # seed별 앙상블 (총 5회)
    SEEDS = [40, 41, 42, 43, 44]
    # 저장변수 초기화
    y_pred_candidates = np.zeros(len(X_predict))
    
    for SEED in SEEDS:

        xgb = XGBClassifier(learning_rate=0.1, max_depth=8, n_estimators=2650,
                            eval_metric='aucpr', colsample_bytree=0.8, subsample=0.8,
                            use_label_encoder=False, min_child_weight=4, n_jobs=-1,
                            booster='gbtree', importance_type='gain',
                            objective = 'binary:logistic', random_state=SEED)

        xgb.fit(X_train, y_train)        
        y_pred = xgb.predict(X_predict)
        # 예측결과(5회) 저장
        y_pred_candidates += y_pred
    
    y_pred_candidates = y_pred_candidates > 2.5
    y_pred = y_pred_candidates.astype(np.int64)

if b_lgbm_active:
    y_pred = lgbm.predict(X_predict)
if b_stack_active:
    y_pred = stack.predict(X_predict)

# 데이터 건수 확인
print('예측 대상 건수 = ', len(X_predict), ', 예측 결과 건수 = ', len(y_pred))

"""**예측 결과 답안지 제출**"""

df_P.head()

df_P.info()

"""> ### 4) 답안지 제출"""

# 결과 제출 답안지 불러오기
ds_sheet = "CDS_submission.csv"
df_sheet = pd.read_csv(ds_sheet)
df_sheet.drop('CHURN', axis=1, inplace=True)
df_sheet.info()

# 답안지에 답안 표기
df_result = df_P.loc[:,('uno','registerdate','productcode')]
df_result['KEY']   = df_result['uno'] + '|' + df_result['registerdate'].dt.strftime('%y-%m-%d %I:%M:%S') + '|' + df_result['productcode']   # 판다스 strftime()
df_result['CHURN'] = pd.DataFrame(y_pred)
df_result = df_result.loc[:,('KEY','CHURN')]
df_answer_sheet = pd.merge(df_sheet, df_result, on='KEY', how='left')
df_answer_sheet.info()

# 답안지 제출 파일 생성하기
ds_answer_sheet = "CDS_submission_Beyond AI for LOVE_6차.csv"
df_answer_sheet.to_csv(ds_answer_sheet, index=False, encoding='utf8')

"""# End"""